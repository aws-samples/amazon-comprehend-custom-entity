{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1099b04396475b6a0143fa303da9fa44ad87b660"
   },
   "source": [
    "# Custom negative custom classifier.\n",
    "\n",
    "This notebook covers how to prepare a training dataset for a negative custom classifier in Amazon Comprehend leveraging the custom keywords that were generated from our word2vec model. \n",
    "\n",
    "We will build a custom negative classifier based on keywords semantically similar to the word \"frustrated\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::202860692096:role/service-role/AmazonSageMaker-ExecutionRole-20180529T141286\n"
     ]
    }
   ],
   "source": [
    "# library imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import csv\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "comprehend = boto3.client('comprehend')\n",
    "\n",
    "# Specify S3 bucket and prefix that you want to use for model data\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = 'data-phi'\n",
    "prefix = 'comprehend-custom-entity'\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54e810d8b9c1936c8569093badabc4d7b25ea881"
   },
   "source": [
    "In this example we will re-use the dataset that we wrangled and filtered for the telco domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "9365c16e4481ec49f5c084f7c3b0cf50dd55047f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32716, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@sprintcare is the worst customer service | @1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sprintcare is the worst customer service | @1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@sprintcare is the worst customer service | @1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@115714 y’all lie about your “great” connectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@115714 whenever I contact customer support, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @sprintcare is the worst customer service | @1...\n",
       "1  @sprintcare is the worst customer service | @1...\n",
       "2  @sprintcare is the worst customer service | @1...\n",
       "3  @115714 y’all lie about your “great” connectio...\n",
       "4  @115714 whenever I contact customer support, t..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames=['text'] \n",
    "tweets = pd.read_csv('./data/tweet_telco.csv',encoding='utf-8',names=colnames, header=None)\n",
    "print(tweets.shape)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "845eba8749f15e1e2b10aa43414f40860259f4e0"
   },
   "source": [
    "<a id='data-wrangling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create our dataset we need to provide an entity list for our new class named NEGATIVITY.\n",
    "\n",
    "In order to find relevant entities, we will be using our custom word2vec model to find semantically similar words to \"frustrated\". See the blazingtext_word2vec_telco_tweets.ipynb notebook for generating keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "tweets['match']=tweets['text'].str.contains(r'(Really|cheated|annoyed|unhelpful|frustrated|upset|unhappy|angry|badly|bad|dissatisfied|disappointed|disgusted)', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's add another column with our class label. This is required part of the Amazon Comprehend training dataset.\n",
    "\n",
    "More information can be found here.\n",
    "\n",
    "https://docs.aws.amazon.com/comprehend/latest/dg/cer-entity-list.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[tweets['match'] == True, 'label'] = 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our training and test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:5: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "training_file = './data/negative_classifier_train.csv'\n",
    "tweets.loc[tweets['label'] == 'negative', ['label', 'text']].to_csv(training_file, encoding='utf-8', index=False)\n",
    "\n",
    "test_file = './data/telco_negative_test.csv'\n",
    "tweets['text'].tail(10000).to_csv(test_file, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "s3_train_key = prefix + \"/train/negative_classifier_train.csv\" \n",
    "s3_test_key = prefix + \"/test/telco_negative_test.csv\"\n",
    "\n",
    "upload_to_s3(s3_train_key, training_file)\n",
    "upload_to_s3(s3_test_key, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded training data location: s3://data-phi/comprehend-custom-entity/comprehend-custom-entity/train/negative_classifier_train.csv\n"
     ]
    }
   ],
   "source": [
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, prefix, s3_train_key)\n",
    "s3_test_job = 's3://{}/{}/{}'.format(bucket, prefix, s3_test_key)\n",
    "s3_output_job = 's3://{}/{}/{}'.format(bucket, prefix, '/output/train_job')\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model\n",
    "\n",
    "I am going to use the console to submit our custom entity recognizer job. Look at the first notebook for details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job = comprehend.create_document_classifier(\n",
    "    DocumentClassifierName='Custom-Negative-Classifier',\n",
    "    DataAccessRoleArn=role,\n",
    "    InputDataConfig={\n",
    "        'S3Uri': s3_train_data\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': s3_output_job\n",
    "    },\n",
    "    LanguageCode='en'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"DocumentClassifierArn\": \"arn:aws:comprehend:us-east-1:202860692096:document-classifier/Custom-Negative-Classifier\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"9b4eb346-8806-4256-b7a9-28ec0ec23fda\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"x-amzn-requestid\": \"9b4eb346-8806-4256-b7a9-28ec0ec23fda\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"116\",\n",
      "      \"date\": \"Sat, 09 Nov 2019 16:37:04 GMT\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(training_job, indent=2, default=str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobArn = training_job['DocumentClassifierArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"DocumentClassifierProperties\": {\n",
      "    \"DocumentClassifierArn\": \"arn:aws:comprehend:us-east-1:202860692096:document-classifier/Custom-Negative-Classifier\",\n",
      "    \"LanguageCode\": \"en\",\n",
      "    \"Status\": \"TRAINED\",\n",
      "    \"SubmitTime\": \"2019-11-09 16:37:04.860000+00:00\",\n",
      "    \"EndTime\": \"2019-11-09 16:49:47.654000+00:00\",\n",
      "    \"TrainingStartTime\": \"2019-11-09 16:40:10.652000+00:00\",\n",
      "    \"TrainingEndTime\": \"2019-11-09 16:48:23.110000+00:00\",\n",
      "    \"InputDataConfig\": {\n",
      "      \"S3Uri\": \"s3://data-phi/comprehend-custom-entity/train/negative_classifier_training.csv\"\n",
      "    },\n",
      "    \"OutputDataConfig\": {\n",
      "      \"S3Uri\": \"s3://data-phi/comprehend-custom-entity/test/custom_classifier_output/202860692096-CLR-ac06c53a0c7b058278df0184069025bf/output/output.tar.gz\"\n",
      "    },\n",
      "    \"ClassifierMetadata\": {\n",
      "      \"NumberOfLabels\": 1,\n",
      "      \"NumberOfTrainedDocuments\": 1311,\n",
      "      \"NumberOfTestDocuments\": 145,\n",
      "      \"EvaluationMetrics\": {\n",
      "        \"Accuracy\": 1.0,\n",
      "        \"Precision\": 1.0,\n",
      "        \"Recall\": 1.0,\n",
      "        \"F1Score\": 1.0\n",
      "      }\n",
      "    },\n",
      "    \"DataAccessRoleArn\": \"arn:aws:iam::202860692096:role/service-role/AmazonSageMaker-ExecutionRole-20180529T141286\"\n",
      "  },\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"f4df3356-d3b3-482c-a598-4f4eb0561895\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"x-amzn-requestid\": \"f4df3356-d3b3-482c-a598-4f4eb0561895\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"985\",\n",
      "      \"date\": \"Sat, 09 Nov 2019 20:08:25 GMT\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(comprehend.describe_document_classifier(\n",
    "        DocumentClassifierArn = jobArn\n",
    "    ), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom entity recognizer: TRAINING\n",
      "Custom entity recognizer: TRAINING\n",
      "Custom entity recognizer: TRAINING\n",
      "Custom entity recognizer: TRAINING\n",
      "Custom entity recognizer: TRAINING\n",
      "Custom entity recognizer: TRAINING\n",
      "Custom entity recognizer: TRAINING\n",
      "Custom entity recognizer: TRAINING\n",
      "Custom entity recognizer: TRAINED\n"
     ]
    }
   ],
   "source": [
    "jobArn = training_job['DocumentClassifierArn']\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_custom_classifier = comprehend.describe_document_classifier(\n",
    "        DocumentClassifierArn = jobArn\n",
    "    )\n",
    "    status = describe_custom_classifier[\"DocumentClassifierProperties\"][\"Status\"]\n",
    "    print(\"Custom entity recognizer: {}\".format(status))\n",
    "    \n",
    "    if status == \"TRAINED\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_output = describe_custom_classifier[\"DocumentClassifierProperties\"][\"OutputDataConfig\"][\"S3Uri\"]\n",
    "print(job_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = 's3://{}/'.format(bucket)\n",
    "job_key = os.path.relpath(job_output, path_prefix)\n",
    "print(job_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the test output to local machine\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket).download_file(job_key, './classifier/output.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/\r\n",
      "output/confusion_matrix.json\r\n"
     ]
    }
   ],
   "source": [
    "!tar xvzf ./classifier/output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'confusion_matrix': [[145]], 'labels': ['negative'], 'type': 'multi_class', 'all_labels': ['negative']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./classifier/output/confusion_matrix.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our custom entity model\n",
    "\n",
    "Let's invoke the Comprehend API to run our test job from the test file we prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
