{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1099b04396475b6a0143fa303da9fa44ad87b660"
   },
   "source": [
    "# Custom negative custom classifier.\n",
    "\n",
    "This notebook covers how to prepare a training dataset for a negative custom classifier in Amazon Comprehend leveraging the custom keywords that were generated from our word2vec model. \n",
    "\n",
    "We will build a custom negative classifier based on keywords semantically similar to the word \"frustrated\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::202860692096:role/service-role/AmazonSageMaker-ExecutionRole-20180529T141286\n"
     ]
    }
   ],
   "source": [
    "# library imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import csv\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime \n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "comprehend = boto3.client('comprehend')\n",
    "\n",
    "# Specify S3 bucket and prefix that you want to use for model data\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = 'data-phi'\n",
    "prefix = 'comprehend-custom-entity'\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54e810d8b9c1936c8569093badabc4d7b25ea881"
   },
   "source": [
    "In this example we will re-use the dataset that we wrangled and filtered for the telco domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "_uuid": "9365c16e4481ec49f5c084f7c3b0cf50dd55047f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32716, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@sprintcare is the worst customer service | @1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sprintcare is the worst customer service | @1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@sprintcare is the worst customer service | @1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@115714 y’all lie about your “great” connectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@115714 whenever I contact customer support, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @sprintcare is the worst customer service | @1...\n",
       "1  @sprintcare is the worst customer service | @1...\n",
       "2  @sprintcare is the worst customer service | @1...\n",
       "3  @115714 y’all lie about your “great” connectio...\n",
       "4  @115714 whenever I contact customer support, t..."
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames=['text'] \n",
    "tweets = pd.read_csv('./data/tweet_telco.csv',encoding='utf-8',names=colnames, header=None)\n",
    "print(tweets.shape)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "845eba8749f15e1e2b10aa43414f40860259f4e0"
   },
   "source": [
    "<a id='data-wrangling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create our dataset we need to label the dataset.\n",
    "\n",
    "In order to find relevant records, we will be using our custom word2vec model to find semantically similar words to \"frustrated\". See the blazingtext_word2vec_telco_tweets.ipynb notebook for generating keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "tweets['match_negative']=tweets['text'].str.contains(r'(Really|cheated|annoyed|unhelpful|frustrated|upset|unhappy|angry|badly|bad|dissatisfied|disappointed|disgusted)', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "tweets['match_positive']=tweets['text'].str.contains(r'(Awesome|AWESOME|Awesome!|Yay!|Hero|Whoop|#YouRock!|Super|Awww!)', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's add another column with our class label. This is required part of the Amazon Comprehend training dataset.\n",
    "\n",
    "More information can be found here.\n",
    "\n",
    "https://docs.aws.amazon.com/comprehend/latest/dg/cer-entity-list.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[tweets['match_negative'] == True, 'label'] = 'NEGATIVE'\n",
    "tweets.loc[tweets['match_positive'] == True, 'label'] = 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>match_negative</th>\n",
       "      <th>match_positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NEGATIVE</th>\n",
       "      <td>1445</td>\n",
       "      <td>1445</td>\n",
       "      <td>1445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POSITIVE</th>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text  match_negative  match_positive\n",
       "label                                         \n",
       "NEGATIVE  1445            1445            1445\n",
       "POSITIVE   254             254             254"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our training and test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = 'negative_classifier_train.csv'\n",
    "tweets.loc[tweets['label'].notnull(), ['label', 'text']].to_csv(training_file, encoding='utf-8', index=False)\n",
    "\n",
    "#test_file = 'telco_negative_test.csv'\n",
    "#tweets['text'].tail(10000).to_csv(test_file, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "s3_train_key = prefix + \"/train\"\n",
    "s3_test_key = prefix + \"/test\"\n",
    "\n",
    "upload_to_s3(s3_train_key, training_file)\n",
    "upload_to_s3(s3_test_key, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded training data location: s3://data-phi/comprehend-custom-entity/train/negative_classifier_train.csv\n"
     ]
    }
   ],
   "source": [
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, s3_train_key, training_file)\n",
    "s3_test_job = 's3://{}/{}/{}'.format(bucket, s3_test_key, test_file)\n",
    "s3_output_job = 's3://{}/{}/{}'.format(bucket, prefix, 'output/train_job')\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1574303977\n"
     ]
    }
   ],
   "source": [
    "print(str(datetime.datetime.now().strftime(\"%s\")))\n",
    "#dt.strftime(\"%s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc = str(datetime.datetime.now().strftime(\"%s\"))\n",
    "training_job = comprehend.create_document_classifier(\n",
    "    DocumentClassifierName='Custom-Negative-Classifier-'+ utc,\n",
    "    DataAccessRoleArn=role,\n",
    "    InputDataConfig={\n",
    "        'S3Uri': s3_train_data\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': s3_output_job\n",
    "    },\n",
    "    LanguageCode='en'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"DocumentClassifierArn\": \"arn:aws:comprehend:us-east-1:202860692096:document-classifier/Custom-Negative-Classifier-1574303978\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"c2b614f1-f423-47ec-8bed-317cfd6f4309\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"x-amzn-requestid\": \"c2b614f1-f423-47ec-8bed-317cfd6f4309\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"127\",\n",
      "      \"date\": \"Thu, 21 Nov 2019 02:39:37 GMT\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(training_job, indent=2, default=str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobArn = training_job['DocumentClassifierArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"DocumentClassifierProperties\": {\n",
      "    \"DocumentClassifierArn\": \"arn:aws:comprehend:us-east-1:202860692096:document-classifier/Custom-Negative-Classifier-1574303978\",\n",
      "    \"LanguageCode\": \"en\",\n",
      "    \"Status\": \"SUBMITTED\",\n",
      "    \"SubmitTime\": \"2019-11-21 02:39:38.615000+00:00\",\n",
      "    \"InputDataConfig\": {\n",
      "      \"S3Uri\": \"s3://data-phi/comprehend-custom-entity/train/negative_classifier_train.csv\"\n",
      "    },\n",
      "    \"OutputDataConfig\": {\n",
      "      \"S3Uri\": \"s3://data-phi/comprehend-custom-entity/output/train_job/202860692096-CLR-dca23527a44c92eafbacbd416e3ca928/output/output.tar.gz\"\n",
      "    },\n",
      "    \"DataAccessRoleArn\": \"arn:aws:iam::202860692096:role/service-role/AmazonSageMaker-ExecutionRole-20180529T141286\"\n",
      "  },\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"a93f351a-862c-4368-8bca-e0ecc12fa070\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"x-amzn-requestid\": \"a93f351a-862c-4368-8bca-e0ecc12fa070\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"627\",\n",
      "      \"date\": \"Thu, 21 Nov 2019 02:39:43 GMT\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(comprehend.describe_document_classifier(\n",
    "        DocumentClassifierArn = jobArn\n",
    "    ), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom classifier: SUBMITTED\n",
      "Custom classifier: TRAINING\n",
      "Custom classifier: TRAINING\n",
      "Custom classifier: TRAINING\n",
      "Custom classifier: TRAINING\n",
      "Custom classifier: TRAINING\n",
      "Custom classifier: TRAINING\n",
      "Custom classifier: TRAINING\n",
      "Custom classifier: TRAINING\n",
      "Custom classifier: TRAINING\n",
      "Custom classifier: TRAINING\n",
      "Custom classifier: TRAINING\n",
      "Custom classifier: TRAINING\n",
      "Custom classifier: TRAINED\n"
     ]
    }
   ],
   "source": [
    "jobArn = training_job['DocumentClassifierArn']\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_custom_classifier = comprehend.describe_document_classifier(\n",
    "        DocumentClassifierArn = jobArn\n",
    "    )\n",
    "    status = describe_custom_classifier[\"DocumentClassifierProperties\"][\"Status\"]\n",
    "    print(\"Custom classifier: {}\".format(status))\n",
    "    \n",
    "    if status == \"TRAINED\" or status == \"IN_ERROR\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://data-phi/comprehend-custom-entity/output/train_job/202860692096-CLR-dca23527a44c92eafbacbd416e3ca928/output/output.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#Retrieve the S3URI from the model output and create jobkey variable.\n",
    "job_output = describe_custom_classifier[\"DocumentClassifierProperties\"][\"OutputDataConfig\"][\"S3Uri\"]\n",
    "path_prefix = 's3://{}/'.format(bucket)\n",
    "job_key = os.path.relpath(job_output, path_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the model metrics\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket).download_file(job_key, './output.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/\r\n",
      "output/confusion_matrix.json\r\n"
     ]
    }
   ],
   "source": [
    "#Unpack the gzip file\n",
    "!tar xvzf ./output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      142,\n",
      "      2\n",
      "    ],\n",
      "    [\n",
      "      7,\n",
      "      18\n",
      "    ]\n",
      "  ],\n",
      "  \"labels\": [\n",
      "    \"NEGATIVE\",\n",
      "    \"POSITIVE\"\n",
      "  ],\n",
      "  \"type\": \"multi_class\",\n",
      "  \"all_labels\": [\n",
      "    \"NEGATIVE\",\n",
      "    \"POSITIVE\",\n",
      "    \"label\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./output/confusion_matrix.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "print(json.dumps(data, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            NEGATIVE  POSITIVE\n",
    "NEGATIVE    142        2\n",
    "POSITIVE    7          18"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
